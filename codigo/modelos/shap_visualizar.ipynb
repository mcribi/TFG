{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "38a9373a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from monai.networks.nets import DenseNet121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1535be2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#config\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3737cb07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Modelo cargado en cuda\n"
     ]
    }
   ],
   "source": [
    "# modelo\n",
    "modelo_path = \"./modelo_densenet_256_gradcamConID_cv_resize_small_hu_m300_1400_separadas_bs4_lr0.001_bs4_lr0.001_fold2.pth\"\n",
    "\n",
    "def build_model():\n",
    "    return DenseNet121(\n",
    "        spatial_dims=3,\n",
    "        in_channels=1,\n",
    "        out_channels=2,\n",
    "        dropout_prob=0.5\n",
    "    )\n",
    "\n",
    "model = build_model()\n",
    "model.load_state_dict(torch.load(modelo_path, map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\" Modelo cargado en\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "70948bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import zoom\n",
    "\n",
    "def upsample_to_64(vol_8):\n",
    "    \"\"\"\n",
    "    Upsamplea un volumen (8,8,8) a (64,64,64) con interpolación lineal.\n",
    "    \"\"\"\n",
    "    return zoom(vol_8, (8,8,8), order=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cab667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# funcion de prediccion\n",
    "def predict_fn(x_numpy):\n",
    "    batch_size = x_numpy.shape[0]\n",
    "    upsampled_batch = []\n",
    "    for i in range(batch_size):\n",
    "        vol_8 = x_numpy[i].reshape(8,8,8)\n",
    "        vol_64 = upsample_to_64(vol_8)\n",
    "        upsampled_batch.append(vol_64)\n",
    "    \n",
    "    upsampled_batch = np.stack(upsampled_batch, axis=0)\n",
    "    x_tensor = torch.tensor(upsampled_batch, dtype=torch.float32).unsqueeze(1).to(device)  # (batch, 1, 64,64,64)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(x_tensor)\n",
    "        probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0587aa7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volumen shape: (64, 64, 64)\n",
      " Downsampling volumen 64x64x64 -> 8x8x8 (promedio por bloque)\n",
      " Nuevo shape: (8, 8, 8)\n",
      " Input para SHAP: (1, 512)\n",
      " Explainer inicializado\n",
      " Calculando SHAP values (puede tardar)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "577257e8adda479a995100cc8d6311b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " SHAP values calculados\n",
      " Verificando shapes SHAP:\n",
      "shap_values[0].shape: (512, 2)\n",
      " Shapes por clase: (512,) (512,)\n"
     ]
    }
   ],
   "source": [
    "# volume\n",
    "volume_path = \"../../volumenes/preprocesados/preprocesamientos_chiquitos/resize_mini_hu_m300_1400_separadas/npy/images/88HASH.npy\"\n",
    "volume = np.load(volume_path)   # (64, 64, 64)\n",
    "print(\" Volumen shape:\", volume.shape)\n",
    "\n",
    "# downsample\n",
    "print(\" Downsampling volumen 64x64x64 -> 8x8x8 (promedio por bloque)\")\n",
    "downsampled = volume.reshape(8,8,8,8,8,8).mean(axis=(1,3,5))\n",
    "print(\" Nuevo shape:\", downsampled.shape)\n",
    "\n",
    "# prep,(samples, features)\n",
    "input_vector = downsampled.flatten().reshape(1, -1)  # (1, 512)\n",
    "print(\" Input para SHAP:\", input_vector.shape)\n",
    "\n",
    "\n",
    "background = np.zeros_like(input_vector)\n",
    "\n",
    "#shap explainer\n",
    "explainer = shap.KernelExplainer(predict_fn, background)\n",
    "print(\" Explainer inicializado\")\n",
    "\n",
    "print(\" Calculando SHAP values (puede tardar)...\")\n",
    "shap_values = explainer.shap_values(input_vector, nsamples=100)\n",
    "print(\" SHAP values calculados\")\n",
    "\n",
    "# reshape\n",
    "print(\" Verificando shapes SHAP:\")\n",
    "print(\"shap_values[0].shape:\", shap_values[0].shape)\n",
    "\n",
    "# extraemos columnas:\n",
    "shap_values_class0 = shap_values[0][:, 0]\n",
    "shap_values_class1 = shap_values[0][:, 1]\n",
    "\n",
    "print(\" Shapes por clase:\", shap_values_class0.shape, shap_values_class1.shape)\n",
    "\n",
    "shap_map_class0 = shap_values_class0.reshape(8,8,8)\n",
    "shap_map_class1 = shap_values_class1.reshape(8,8,8)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5960d741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Imagen SHAP guardada en 'shap_explanation_downsampled.png'\n"
     ]
    }
   ],
   "source": [
    "slice_idx = 4\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.title(\"SHAP - Clase 0 (No complicación)\")\n",
    "plt.imshow(shap_map_class0[slice_idx,:,:], cmap='bwr')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title(\"SHAP - Clase 1 (Complicación)\")\n",
    "plt.imshow(shap_map_class1[slice_idx,:,:], cmap='bwr')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"shap_explanation_downsampled.png\")\n",
    "plt.close()\n",
    "print(\" Imagen SHAP guardada en 'shap_explanation_downsampled.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b76bbbf",
   "metadata": {},
   "source": [
    "Hacemos otra prueba:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb9a0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Modelo cargado\n",
      " Volumen original: (64, 64, 64)\n",
      "Downsampling 64x64x64 -> 16x16x16 con bloques 4x4x4\n",
      "Shape downsampled: (16, 16, 16)\n",
      "Input SHAP: (1, 4096)\n",
      " SHAP explainer creado\n",
      " Calculando SHAP values (puede tardar)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b92b455b8d0042728e82948af9cea2b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " SHAP calculado\n",
      " Shapes SHAP: (4096, 2)\n",
      "SHAP maps reconstruidos (16x16x16)\n",
      "Upsampled a: (64, 64, 64)\n",
      " Volúmenes SHAP guardados como .npy\n",
      " Todas las slices guardadas como PNG en: ./shap_slices_output\n"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from monai.networks.nets import DenseNet121\n",
    "from scipy.ndimage import zoom\n",
    "import os\n",
    "\n",
    "# config\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "modelo_path = \"./modelo_densenet_256_gradcamConID_cv_resize_small_hu_m300_1400_separadas_bs4_lr0.001_bs4_lr0.001_fold2.pth\"\n",
    "volume_path = \"../../volumenes/preprocesados/preprocesamientos_chiquitos/resize_mini_hu_m300_1400_separadas/npy/images/88HASH.npy\"\n",
    "output_dir = \"./shap_slices_output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# load model\n",
    "def build_model():\n",
    "    return DenseNet121(\n",
    "        spatial_dims=3,\n",
    "        in_channels=1,\n",
    "        out_channels=2,\n",
    "        dropout_prob=0.5\n",
    "    )\n",
    "\n",
    "model = build_model()\n",
    "model.load_state_dict(torch.load(modelo_path, map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(\" Modelo cargado\")\n",
    "\n",
    "#load volume\n",
    "volume = np.load(volume_path)\n",
    "print(\" Volumen original:\", volume.shape)\n",
    "\n",
    "# Normalize\n",
    "volume = (volume - volume.min()) / (volume.max() - volume.min() + 1e-8)\n",
    "\n",
    "#Downsampling\n",
    "print(\"Downsampling 64x64x64 -> 16x16x16 con bloques 4x4x4\")\n",
    "downsampled = volume.reshape(16,4,16,4,16,4).mean(axis=(1,3,5))\n",
    "print(\"Shape downsampled:\", downsampled.shape)\n",
    "\n",
    "input_vector = downsampled.flatten().reshape(1, -1)\n",
    "print(\"Input SHAP:\", input_vector.shape)\n",
    "\n",
    "# upsampling\n",
    "def upsample_to_64(vol_16):\n",
    "    return zoom(vol_16, (4,4,4), order=1)\n",
    "\n",
    "# prediction func\n",
    "def predict_fn(x_numpy):\n",
    "    batch_size = x_numpy.shape[0]\n",
    "    upsampled_batch = []\n",
    "    for i in range(batch_size):\n",
    "        vol_16 = x_numpy[i].reshape(16,16,16)\n",
    "        vol_64 = upsample_to_64(vol_16)\n",
    "        upsampled_batch.append(vol_64)\n",
    "    upsampled_batch = np.stack(upsampled_batch, axis=0)\n",
    "    x_tensor = torch.tensor(upsampled_batch, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(x_tensor)\n",
    "        probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
    "    return probs\n",
    "\n",
    "# background\n",
    "background = np.zeros_like(input_vector)\n",
    "\n",
    "# explainer\n",
    "explainer = shap.KernelExplainer(predict_fn, background)\n",
    "print(\" SHAP explainer creado\")\n",
    "\n",
    "print(\" Calculando SHAP values (puede tardar)...\")\n",
    "shap_values = explainer.shap_values(input_vector, nsamples=300)\n",
    "print(\" SHAP calculado\")\n",
    "\n",
    "#extremos clases\n",
    "print(\" Shapes SHAP:\", shap_values[0].shape)\n",
    "\n",
    "# SHAP devuelve (512, 2)\n",
    "shap_values_class0 = shap_values[0][:, 0]\n",
    "shap_values_class1 = shap_values[0][:, 1]\n",
    "\n",
    "shap_map_class0 = shap_values_class0.reshape(16,16,16)\n",
    "shap_map_class1 = shap_values_class1.reshape(16,16,16)\n",
    "\n",
    "print(\"SHAP maps reconstruidos (16x16x16)\")\n",
    "\n",
    "#Upsample a 64x64x64\n",
    "shap_map_class0_up = zoom(shap_map_class0, (4,4,4), order=1)\n",
    "shap_map_class1_up = zoom(shap_map_class1, (4,4,4), order=1)\n",
    "print(\"Upsampled a:\", shap_map_class0_up.shape)\n",
    "\n",
    "# guardamos volumen completo\n",
    "np.save(os.path.join(output_dir, \"shap_class0_volume.npy\"), shap_map_class0_up)\n",
    "np.save(os.path.join(output_dir, \"shap_class1_volume.npy\"), shap_map_class1_up)\n",
    "print(\" Volúmenes SHAP guardados como .npy\")\n",
    "\n",
    "#guardmaos todas las slices\n",
    "for z in range(64):\n",
    "    plt.imsave(\n",
    "        os.path.join(output_dir, f\"class0_slice_{z}.png\"),\n",
    "        shap_map_class0_up[z,:,:],\n",
    "        cmap='bwr'\n",
    "    )\n",
    "    plt.imsave(\n",
    "        os.path.join(output_dir, f\"class1_slice_{z}.png\"),\n",
    "        shap_map_class1_up[z,:,:],\n",
    "        cmap='bwr'\n",
    "    )\n",
    "print(\" Todas las slices guardadas como PNG en:\", output_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
